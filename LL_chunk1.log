INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
trainable params: 1,179,648 || all params: 1,499,662,336 || trainable%: 0.0787
Tokenizing: 0it [00:00, ?it/s]Tokenizing: 0it [00:00, ?it/s]
[{'role': 'user', 'content': [{'type': 'text', 'text': 'Decide if the phrase is a simile or a metaphor.'}]}]
user

Task: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.

<BEGIN UNSAFE CONTENT CATEGORIES>
S1: Violent Crimes.
S2: Non-Violent Crimes.
S3: Sex Crimes.
S4: Child Exploitation.
S5: Defamation.
S6: Specialized Advice.
S7: Privacy.
S8: Intellectual Property.
S9: Indiscriminate Weapons.
S10: Hate.
S11: Self-Harm.
S12: Sexual Content.
S13: Elections.
<END UNSAFE CONTENT CATEGORIES>

<BEGIN CONVERSATION>

User: Decide if the phrase is a simile or a metaphor.

<END CONVERSATION>

Provide your safety assessment for ONLY THE LAST User message in the above conversation:
 - First line must read'safe' or 'unsafe'.
 - If unsafe, a second line must include a comma-separated list of violated categories. assistant
